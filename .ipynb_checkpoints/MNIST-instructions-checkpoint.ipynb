{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1598e8a-f783-4530-8fd8-55a05b9a08b9",
   "metadata": {},
   "source": [
    "# Trabalho Computacional 2. Perceptron Multicamada no problema MNIST\n",
    "\n",
    "## 1. Introdução e Preparação\n",
    "\n",
    "Faremos uso do MLP para o problema de classificação de dígitos manuscritos MNIST. O código apresentado nestas instruções devem ser visto como sugestão, mas o trabalho deve necessariamente estar em python, na forma de um \"notebook\", e implementar o perceptron multicamada com as características dadas para resolver o problema dado. \n",
    "\n",
    "Além das sugestões dadas aqui, observe o caderno de implementação do MLP no problema Fashion_MNIST que estudamos em sala. \n",
    "\n",
    "Começamos com os módulos necessários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3768c65",
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809751a",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "Não é necessário desta vez usar um arquivo em disco, porque a base de dados MNIST está disponível como um dos conjuntos de dados do pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b983c2",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "class MNIST(d2l.DataModule):  #@save\n",
    "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
    "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        trans = transforms.Compose([transforms.Resize(resize),\n",
    "                                    transforms.ToTensor()])\n",
    "        self.train = torchvision.datasets.MNIST(\n",
    "            root=self.root, train=True, transform=trans, download=True)\n",
    "        self.val = torchvision.datasets.MNIST(\n",
    "            root=self.root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e7095",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Há 60000 exemplos de treinamento e 10000 de validação. A entrada, em cada exemplo, é uma matriz 28x28 de pixels. As imagens originais manuscritas foram pré-processadas (normalizadas em tamanho e centralizadas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161babf5",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST()\n",
    "print(len(dataset.train), len(dataset.val))\n",
    "print(dataset.train.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20025758",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "Abaixo um código, do d2l, para ler um minibatch de um dado tamanho. Em cada chamada, ele nos dá o tensor de entrada X e a classe desejada y. \n",
    "Ele também embaralha os dados no momento do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73c5e82",
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MNIST)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    data = self.train if train else self.val\n",
    "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
    "                                       num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db8062",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "Vamos usar isso com um minibatch e ver as dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957eda7b",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.float32 torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(dataset.train_dataloader()))\n",
    "print(X.shape, X.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb84e8-3aca-417b-9cdb-66f928c71c89",
   "metadata": {},
   "source": [
    "Você pode estranhar a princípio as dimensões de X. São 64 imagens 28x28, mas por que a dimensão unitária adicional? Lembre-se que, em geral, imagens têm 3 canais de cor. Estas imagens são em tons de cinza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b4725",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "## 2. Modelos e Arquiteturas\n",
    "\n",
    "Para a implementação, baseie-se na classe MLP definida no caderno que vimos em sala de aula. A primeira camada deve ser do tipo \"Flatten\" como no outro exemplo. Ela transforma a matriz 28x28 em um vetor unidimensional de tamanho 784. Neste trabalho, vocês farão algumas variações de arquitetura, funções de ativação e algoritmos de treinamento, indo das técnicas usadas nas primeiras aplicações do Perceptron nos anos 80/90 às atuais. Para cada uma delas, avalie a acurácia, sobre o arquivo de validação, do modelo treinado. \n",
    "\n",
    "Eu não estou sugerindo que cada modificação introduzida vai necessariamente melhorar o modelo em todas as ocasiões. Lembre-se, também, que um valor melhor em um único treinamento pode não ter significância estatística. **Haverá bonificação se essas análises foram feitas para mais de uma rodada de treinamento (com valores de inicialização aleatória diferentes).** \n",
    "\n",
    "### 2a) Perceptron com uma camada escondida, função de ativação sigmoide e otimização por descida de gradiente.\n",
    "\n",
    "Comecemos com um MLP com uma camada escondida de 128 neurônios. Usaremos a **função logística** (sigmoide) como função de ativação na camada escondida e softmax na camada de saída (lembre que a camada de saída pode na verdade ser declarada linear, pois o pytorch já aplica o softmax na implementação da função de custo de entropia cruzada, como vimos em sala). Mantenha entropia cruzada como função de custo e a descida simples de gradiente como otimizador. \n",
    "\n",
    "### 2b) Otimizador Adam\n",
    "\n",
    "O gradiente simples tem um inconveniente grave: sabemos que melhoramos o desempenho do classificador se acompanharmos o gradiente em um \"pequeno passo\", mas não é fácil determinar que passo deveria ser esse. E sabemos que ele deveria variar ao longo do treinamento. Várias propostas para uma adaptação da taxa de aprendizado foram feitas, mas a que acabou se tornando padrão é o chamado otimizador Adam (Kingma and Ba, 2014). O nome vem de \"Estimativa adaptativa de momentos\". Acompanhando a média e variância do gradiente entre atualizações, ele consegue adaptar a taxa de aprendizado, e torná-la diferente para cada parâmetro. \n",
    "\n",
    "Redefina a função `configure_optimizers` dentro da classe `MLP` e nela use agora `torch.optim.Adam` como o otimizador. Retreine e reavalie. \n",
    "\n",
    "### 2c) A função Relu\n",
    "\n",
    "Um desenvolvimento um tanto surpreendente foi a descoberta de que uma simples função retificadora (0 se a soma é negativa, função identidade se é positiva) nos dá não-linearidade suficiente para o MLP. Use a **ReLU** como função de ativação na camada oculta. Retreine e reavalie. \n",
    "\n",
    "### 2d) Redes maiores\n",
    "\n",
    "Com esses desenvolvimentos, e a melhoria geral na capacidade de computação, aos poucos foi possível trabalhar com redes cada vez maiores. Experimente uma camada escondida com 256 neurônios, e também uma rede com duas camadas escondidas. Retreine e reavalie. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de94de2-9205-48e9-bf13-c8f25357ff8d",
   "metadata": {},
   "source": [
    "## 3. Visualização \n",
    "\n",
    "Gostaríamos agora de observar alguns dos exemplos mal-classificados. Você pode se basear na função `.visualize` do exemplo visto em sala, mas adaptações são necessárias. Em primeiro lugar, naquela ocasião usamos apenas os erros de um minibatch. Como o desempenho daquele classificador foi relativamente fraco, havia erros em todos os \"minibatches\". O MLP (especialmente os maiores) deve ter um número pequeno de erros, e é possível que não haja nenhuma classificação errada em um dado minibatch. Isso produziria um erro no seu código.  \n",
    "\n",
    "Além disso, queremos talvez visualizar o erro não apenas no último minibatch. Faça um código que passe por todos os minibatches (por exemplo, `for X, y in dataset.val_dataloader()`) e identifique exemplos de classificações incorretas. Ou então, trabalhe diretamente com o objeto MNIST `dataset`. Neste caso, lembre-se que `dataset.train` e `dataset.val` são ambas \"duplas\", contendo as entradas e as saídas de todos os exemplos.  Usar o iterador como `dataset.val_dataloader()` torna-se essencial quando usarmos grandes bases de dados e não pudermos trazer e alocar tudo em memória de uma vez.\n",
    "\n",
    "Há um outro problema na implementação feita em sala. `dataset.visualize` espera uma lista de imagens, e não vai funcionar com uma imagem única. Assim como pode haver minibatches sem erros, pode haver minibatches com um erro a visualizar apenas. Nestes casos talvez as funções `imshow` e `show` do módulo `matplotlib.pyplot` possam ser úteis. \n",
    "\n",
    "Seja como for, mostre alguns padrões erradamente classificados (juntamente com os rótulos esperados e obtidos, como vimos no exemplo em sala de aula). Você acha que é razoável que um humano cometesse esse tipo de erro? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef07eae-667f-4a78-9987-95695e681b6b",
   "metadata": {},
   "source": [
    "## 4. Matriz de confusão\n",
    "\n",
    "Queremos agora observar a matriz de confusão do seu melhor classificador. A função `confusion_matrix` do módulo `sklearn.metrics` faz esse cálculo, mas temos de novo o problema que essa função espera receber todos as classes corretas (`y`) e todas as predições (`preds`) de uma vez. De novo, faça isso para cada minibatch usando o iterador `dataset.val_dataloader()` e depois combine-as corretamente, ou então trabalhe diretamente com as duplas no objeto `dataset`. \n",
    "\n",
    "Obtenha a matriz de confusão (10x10). Observe o resultado. Quais são as \"confusões\" mais comuns para cada classe (ou seja, para cada classe i qual a classe j é a atribuição errada mais comum? Isso faz sentido?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
